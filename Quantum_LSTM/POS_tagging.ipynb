{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quantum-Enhanced LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One field that so far has been poorly explored in Quantum Machine Learning is Natural Language Processing (NLP), the sub-field of Artificial Intelligence that gives computers the ability to read, write and to some extent comprehend written text. \n",
    "\n",
    "As documents are usually presented as sequences of words, historically one of the most successful techniques to manipulate this kind of data has been the Recurrent Neural Network architecture, and in particular a variant called Long Short-Term Memory (LSTM). LSTMs allowed machines to perform translations, classification and intent detection with state-of-the-art accuracy until the advent of Transformer networks. Still, it’s interesting at least from an educational point of view to dig into LSTMs to see what good quantum computing may bring to the field. For a more thorough discussion, please refer to “Quantum Long Short-Term Memory” by Chen, Yoo and Fang (arXiv:2009.01783) and “Recurrent Quantum Neural Networks” by J. Bausch (arXiv:2006.14619)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==0.12.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: aiohttp==3.7.4.post0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.7.4.post0)\n",
      "Requirement already satisfied: appdirs==1.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==3.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: attrs==20.3.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (20.3.0)\n",
      "Requirement already satisfied: autograd==1.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.3)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.9.3)\n",
      "Requirement already satisfied: bs4==0.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.0.1)\n",
      "Requirement already satisfied: cachetools==4.2.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (4.2.1)\n",
      "Requirement already satisfied: certifi==2020.12.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2020.12.5)\n",
      "Requirement already satisfied: cffi==1.14.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.14.5)\n",
      "Requirement already satisfied: chardet==4.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: click==7.1.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (7.1.2)\n",
      "Requirement already satisfied: cryptography==3.4.7 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (3.4.7)\n",
      "Requirement already satisfied: cycler==0.10.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: decorator==4.4.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (4.4.2)\n",
      "Requirement already satisfied: dill==0.3.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (0.3.3)\n",
      "Requirement already satisfied: dlx==1.0.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (1.0.4)\n",
      "Requirement already satisfied: docplex==2.15.194 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (2.15.194)\n",
      "Requirement already satisfied: fastdtw==0.3.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (0.3.4)\n",
      "Requirement already satisfied: fastjsonschema==2.15.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (2.15.0)\n",
      "Requirement already satisfied: fsspec==0.9.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.9.0)\n",
      "Requirement already satisfied: future==0.18.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (0.18.2)\n",
      "Requirement already satisfied: google-auth==1.28.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (1.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (0.4.4)\n",
      "Requirement already satisfied: grpcio==1.37.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (1.37.0)\n",
      "Requirement already satisfied: h5py==3.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (3.1.0)\n",
      "Requirement already satisfied: idna==2.10 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (2.10)\n",
      "Requirement already satisfied: inflection==0.5.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (0.5.1)\n",
      "Requirement already satisfied: joblib==1.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 30)) (1.0.1)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (3.2.0)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (1.3.1)\n",
      "Requirement already satisfied: lxml==4.6.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (4.6.3)\n",
      "Requirement already satisfied: Markdown==3.3.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (3.3.4)\n",
      "Requirement already satisfied: matplotlib==3.4.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (3.4.1)\n",
      "Requirement already satisfied: more-itertools==8.7.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (8.7.0)\n",
      "Requirement already satisfied: mpmath==1.2.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 37)) (1.2.1)\n",
      "Requirement already satisfied: multidict==5.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (5.1.0)\n",
      "Requirement already satisfied: multitasking==0.0.9 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (0.0.9)\n",
      "Requirement already satisfied: nest-asyncio==1.5.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (1.5.1)\n",
      "Requirement already satisfied: networkx==2.5.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (2.5.1)\n",
      "Requirement already satisfied: nltk==3.6.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (3.6.1)\n",
      "Requirement already satisfied: ntlm-auth==1.5.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (1.5.0)\n",
      "Requirement already satisfied: numpy==1.20.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (1.20.1)\n",
      "Requirement already satisfied: oauthlib==3.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 45)) (3.1.0)\n",
      "Requirement already satisfied: pandas==1.2.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (1.2.3)\n",
      "Requirement already satisfied: PennyLane==0.14.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (0.14.1)\n",
      "Requirement already satisfied: PennyLane-qiskit==0.14.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 48)) (0.14.0)\n",
      "Requirement already satisfied: Pillow==8.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 49)) (8.2.0)\n",
      "Requirement already satisfied: ply==3.11 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (3.11)\n",
      "Requirement already satisfied: protobuf==3.15.7 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 51)) (3.15.7)\n",
      "Requirement already satisfied: psutil==5.8.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 52)) (5.8.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 53)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (0.2.8)\n",
      "Requirement already satisfied: pybind11==2.6.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (2.6.2)\n",
      "Requirement already satisfied: pycparser==2.20 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 56)) (2.20)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 57)) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent==0.17.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 58)) (0.17.3)\n",
      "Requirement already satisfied: python-constraint==1.4.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 59)) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 60)) (2.8.1)\n",
      "Requirement already satisfied: pytorch-lightning==1.2.7 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (1.2.7)\n",
      "Requirement already satisfied: pytz==2021.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 62)) (2021.1)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (5.3.1)\n",
      "Requirement already satisfied: qiskit==0.25.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 64)) (0.25.0)\n",
      "Requirement already satisfied: qiskit-aer==0.8.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (0.8.0)\n",
      "Requirement already satisfied: qiskit-aqua==0.9.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 66)) (0.9.0)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.12.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 67)) (0.12.2)\n",
      "Requirement already satisfied: qiskit-ignis==0.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 68)) (0.6.0)\n",
      "Requirement already satisfied: qiskit-terra==0.17.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 69)) (0.17.0)\n",
      "Requirement already satisfied: Quandl==3.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 70)) (3.6.0)\n",
      "Requirement already satisfied: regex==2021.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 71)) (2021.4.4)\n",
      "Requirement already satisfied: requests==2.25.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 72)) (2.25.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-ntlm==1.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 73)) (1.1.0)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 74)) (1.3.0)\n",
      "Requirement already satisfied: retworkx==0.8.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 75)) (0.8.0)\n",
      "Requirement already satisfied: rsa==4.7.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 76)) (4.7.2)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 77)) (0.24.1)\n",
      "Requirement already satisfied: scipy==1.6.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 78)) (1.6.1)\n",
      "Requirement already satisfied: semantic-version==2.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 79)) (2.6.0)\n",
      "Requirement already satisfied: six==1.15.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 80)) (1.15.0)\n",
      "Requirement already satisfied: sklearn==0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 81)) (0.0)\n",
      "Requirement already satisfied: soupsieve==2.2.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 82)) (2.2.1)\n",
      "Requirement already satisfied: sympy==1.7.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (1.7.1)\n",
      "Requirement already satisfied: tensorboard==2.4.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 84)) (2.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 85)) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 86)) (2.1.0)\n",
      "Requirement already satisfied: toml==0.10.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 87)) (0.10.2)\n",
      "Requirement already satisfied: torch==1.8.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 88)) (1.8.1)\n",
      "Requirement already satisfied: torchmetrics==0.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 89)) (0.2.0)\n",
      "Requirement already satisfied: torchvision==0.9.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 90)) (0.9.1)\n",
      "Requirement already satisfied: tqdm==4.60.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 91)) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 92)) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3==1.26.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 93)) (1.26.4)\n",
      "Requirement already satisfied: websockets==8.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 94)) (8.1)\n",
      "Requirement already satisfied: Werkzeug==1.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 95)) (1.0.1)\n",
      "Requirement already satisfied: yarl==1.6.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 96)) (1.6.3)\n",
      "Requirement already satisfied: yfinance==0.1.55 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 97)) (0.1.55)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in ./venv/lib/python3.8/site-packages (from google-auth==1.28.0->-r requirements.txt (line 24)) (49.2.1)\n",
      "Requirement already satisfied: wheel>=0.26 in ./venv/lib/python3.8/site-packages (from tensorboard==2.4.1->-r requirements.txt (line 84)) (0.37.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from qlstm_pennylane import QLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the possible tags: determinant, noun, verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "ix_to_tag = {i:k for k,i in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below tokenizes the sentence and matches the label to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "Entities: {0: 'DET', 1: 'NN', 2: 'V'}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\n",
    "print(f\"Vocabulary: {word_to_ix}\")\n",
    "print(f\"Entities: {ix_to_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to pass the two sequences through the LSTM, which will output the hidden array of vectors [h_0, h_1, h_2, h_3, h_4], one for each word. A dense layer “head” is attached to the LSTM’s outputs to calculate the probability that each word may be a determinant, noun or verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_qubits=0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        if n_qubits > 0:\n",
    "            print(\"Tagger will use Quantum LSTM\")\n",
    "            self.lstm = QLSTM(embedding_dim, hidden_dim, n_qubits=n_qubits)\n",
    "        else:\n",
    "            print(\"Tagger will use Classical LSTM\")\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_logits, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "hidden_dim = 6\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger will use Classical LSTM\n"
     ]
    }
   ],
   "source": [
    "model_classical = LSTMTagger(embedding_dim, \n",
    "                        hidden_dim, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(tag_to_ix), \n",
    "                        n_qubits=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example from the PyTorch website, we train the two networks (classical and quantum LSTM) for 300 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'acc': []\n",
    "    }\n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        preds = []\n",
    "        targets = []\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            labels = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "            \n",
    "            probs = torch.softmax(tag_scores, dim=-1)\n",
    "            preds.append(probs.argmax(dim=-1))\n",
    "            targets.append(labels)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        corrects = (preds == targets)\n",
    "        accuracy = corrects.sum().float() / float(targets.size(0) )\n",
    "        history['acc'].append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} / {n_epochs}: Loss = {avg_loss:.3f} Acc = {accuracy:.2f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300: Loss = 1.145 Acc = 0.33\n",
      "Epoch 2 / 300: Loss = 1.135 Acc = 0.33\n",
      "Epoch 3 / 300: Loss = 1.126 Acc = 0.33\n",
      "Epoch 4 / 300: Loss = 1.119 Acc = 0.22\n",
      "Epoch 5 / 300: Loss = 1.112 Acc = 0.22\n",
      "Epoch 6 / 300: Loss = 1.106 Acc = 0.33\n",
      "Epoch 7 / 300: Loss = 1.100 Acc = 0.22\n",
      "Epoch 8 / 300: Loss = 1.095 Acc = 0.22\n",
      "Epoch 9 / 300: Loss = 1.090 Acc = 0.33\n",
      "Epoch 10 / 300: Loss = 1.086 Acc = 0.33\n",
      "Epoch 11 / 300: Loss = 1.082 Acc = 0.44\n",
      "Epoch 12 / 300: Loss = 1.079 Acc = 0.44\n",
      "Epoch 13 / 300: Loss = 1.075 Acc = 0.44\n",
      "Epoch 14 / 300: Loss = 1.072 Acc = 0.44\n",
      "Epoch 15 / 300: Loss = 1.069 Acc = 0.44\n",
      "Epoch 16 / 300: Loss = 1.066 Acc = 0.44\n",
      "Epoch 17 / 300: Loss = 1.063 Acc = 0.44\n",
      "Epoch 18 / 300: Loss = 1.061 Acc = 0.44\n",
      "Epoch 19 / 300: Loss = 1.058 Acc = 0.44\n",
      "Epoch 20 / 300: Loss = 1.056 Acc = 0.44\n",
      "Epoch 21 / 300: Loss = 1.053 Acc = 0.44\n",
      "Epoch 22 / 300: Loss = 1.051 Acc = 0.44\n",
      "Epoch 23 / 300: Loss = 1.049 Acc = 0.44\n",
      "Epoch 24 / 300: Loss = 1.047 Acc = 0.44\n",
      "Epoch 25 / 300: Loss = 1.044 Acc = 0.44\n",
      "Epoch 26 / 300: Loss = 1.042 Acc = 0.44\n",
      "Epoch 27 / 300: Loss = 1.040 Acc = 0.44\n",
      "Epoch 28 / 300: Loss = 1.038 Acc = 0.44\n",
      "Epoch 29 / 300: Loss = 1.036 Acc = 0.44\n",
      "Epoch 30 / 300: Loss = 1.033 Acc = 0.44\n",
      "Epoch 31 / 300: Loss = 1.031 Acc = 0.44\n",
      "Epoch 32 / 300: Loss = 1.029 Acc = 0.44\n",
      "Epoch 33 / 300: Loss = 1.027 Acc = 0.44\n",
      "Epoch 34 / 300: Loss = 1.025 Acc = 0.44\n",
      "Epoch 35 / 300: Loss = 1.022 Acc = 0.44\n",
      "Epoch 36 / 300: Loss = 1.020 Acc = 0.44\n",
      "Epoch 37 / 300: Loss = 1.017 Acc = 0.44\n",
      "Epoch 38 / 300: Loss = 1.015 Acc = 0.44\n",
      "Epoch 39 / 300: Loss = 1.012 Acc = 0.44\n",
      "Epoch 40 / 300: Loss = 1.010 Acc = 0.44\n",
      "Epoch 41 / 300: Loss = 1.007 Acc = 0.44\n",
      "Epoch 42 / 300: Loss = 1.005 Acc = 0.44\n",
      "Epoch 43 / 300: Loss = 1.002 Acc = 0.44\n",
      "Epoch 44 / 300: Loss = 0.999 Acc = 0.44\n",
      "Epoch 45 / 300: Loss = 0.996 Acc = 0.44\n",
      "Epoch 46 / 300: Loss = 0.993 Acc = 0.44\n",
      "Epoch 47 / 300: Loss = 0.990 Acc = 0.44\n",
      "Epoch 48 / 300: Loss = 0.987 Acc = 0.44\n",
      "Epoch 49 / 300: Loss = 0.983 Acc = 0.44\n",
      "Epoch 50 / 300: Loss = 0.980 Acc = 0.44\n",
      "Epoch 51 / 300: Loss = 0.977 Acc = 0.44\n",
      "Epoch 52 / 300: Loss = 0.973 Acc = 0.44\n",
      "Epoch 53 / 300: Loss = 0.969 Acc = 0.44\n",
      "Epoch 54 / 300: Loss = 0.965 Acc = 0.44\n",
      "Epoch 55 / 300: Loss = 0.962 Acc = 0.56\n",
      "Epoch 56 / 300: Loss = 0.958 Acc = 0.56\n",
      "Epoch 57 / 300: Loss = 0.953 Acc = 0.56\n",
      "Epoch 58 / 300: Loss = 0.949 Acc = 0.56\n",
      "Epoch 59 / 300: Loss = 0.945 Acc = 0.56\n",
      "Epoch 60 / 300: Loss = 0.940 Acc = 0.56\n",
      "Epoch 61 / 300: Loss = 0.936 Acc = 0.56\n",
      "Epoch 62 / 300: Loss = 0.931 Acc = 0.56\n",
      "Epoch 63 / 300: Loss = 0.926 Acc = 0.56\n",
      "Epoch 64 / 300: Loss = 0.921 Acc = 0.56\n",
      "Epoch 65 / 300: Loss = 0.916 Acc = 0.67\n",
      "Epoch 66 / 300: Loss = 0.911 Acc = 0.67\n",
      "Epoch 67 / 300: Loss = 0.906 Acc = 0.67\n",
      "Epoch 68 / 300: Loss = 0.900 Acc = 0.67\n",
      "Epoch 69 / 300: Loss = 0.895 Acc = 0.67\n",
      "Epoch 70 / 300: Loss = 0.889 Acc = 0.67\n",
      "Epoch 71 / 300: Loss = 0.883 Acc = 0.67\n",
      "Epoch 72 / 300: Loss = 0.877 Acc = 0.67\n",
      "Epoch 73 / 300: Loss = 0.871 Acc = 0.67\n",
      "Epoch 74 / 300: Loss = 0.864 Acc = 0.67\n",
      "Epoch 75 / 300: Loss = 0.858 Acc = 0.67\n",
      "Epoch 76 / 300: Loss = 0.852 Acc = 0.67\n",
      "Epoch 77 / 300: Loss = 0.845 Acc = 0.67\n",
      "Epoch 78 / 300: Loss = 0.838 Acc = 0.67\n",
      "Epoch 79 / 300: Loss = 0.831 Acc = 0.78\n",
      "Epoch 80 / 300: Loss = 0.824 Acc = 0.78\n",
      "Epoch 81 / 300: Loss = 0.817 Acc = 0.78\n",
      "Epoch 82 / 300: Loss = 0.810 Acc = 0.78\n",
      "Epoch 83 / 300: Loss = 0.803 Acc = 0.78\n",
      "Epoch 84 / 300: Loss = 0.795 Acc = 0.78\n",
      "Epoch 85 / 300: Loss = 0.788 Acc = 0.78\n",
      "Epoch 86 / 300: Loss = 0.780 Acc = 0.78\n",
      "Epoch 87 / 300: Loss = 0.772 Acc = 0.78\n",
      "Epoch 88 / 300: Loss = 0.765 Acc = 0.78\n",
      "Epoch 89 / 300: Loss = 0.757 Acc = 0.78\n",
      "Epoch 90 / 300: Loss = 0.749 Acc = 0.78\n",
      "Epoch 91 / 300: Loss = 0.741 Acc = 0.78\n",
      "Epoch 92 / 300: Loss = 0.733 Acc = 0.89\n",
      "Epoch 93 / 300: Loss = 0.725 Acc = 0.89\n",
      "Epoch 94 / 300: Loss = 0.717 Acc = 0.89\n",
      "Epoch 95 / 300: Loss = 0.709 Acc = 0.89\n",
      "Epoch 96 / 300: Loss = 0.701 Acc = 0.89\n",
      "Epoch 97 / 300: Loss = 0.692 Acc = 0.89\n",
      "Epoch 98 / 300: Loss = 0.684 Acc = 0.89\n",
      "Epoch 99 / 300: Loss = 0.676 Acc = 0.89\n",
      "Epoch 100 / 300: Loss = 0.668 Acc = 0.89\n",
      "Epoch 101 / 300: Loss = 0.660 Acc = 0.89\n",
      "Epoch 102 / 300: Loss = 0.651 Acc = 0.89\n",
      "Epoch 103 / 300: Loss = 0.643 Acc = 0.89\n",
      "Epoch 104 / 300: Loss = 0.635 Acc = 1.00\n",
      "Epoch 105 / 300: Loss = 0.627 Acc = 1.00\n",
      "Epoch 106 / 300: Loss = 0.619 Acc = 1.00\n",
      "Epoch 107 / 300: Loss = 0.611 Acc = 1.00\n",
      "Epoch 108 / 300: Loss = 0.602 Acc = 1.00\n",
      "Epoch 109 / 300: Loss = 0.594 Acc = 1.00\n",
      "Epoch 110 / 300: Loss = 0.586 Acc = 1.00\n",
      "Epoch 111 / 300: Loss = 0.578 Acc = 1.00\n",
      "Epoch 112 / 300: Loss = 0.570 Acc = 1.00\n",
      "Epoch 113 / 300: Loss = 0.563 Acc = 1.00\n",
      "Epoch 114 / 300: Loss = 0.555 Acc = 1.00\n",
      "Epoch 115 / 300: Loss = 0.547 Acc = 1.00\n",
      "Epoch 116 / 300: Loss = 0.539 Acc = 1.00\n",
      "Epoch 117 / 300: Loss = 0.532 Acc = 1.00\n",
      "Epoch 118 / 300: Loss = 0.524 Acc = 1.00\n",
      "Epoch 119 / 300: Loss = 0.516 Acc = 1.00\n",
      "Epoch 120 / 300: Loss = 0.509 Acc = 1.00\n",
      "Epoch 121 / 300: Loss = 0.502 Acc = 1.00\n",
      "Epoch 122 / 300: Loss = 0.494 Acc = 1.00\n",
      "Epoch 123 / 300: Loss = 0.487 Acc = 1.00\n",
      "Epoch 124 / 300: Loss = 0.480 Acc = 1.00\n",
      "Epoch 125 / 300: Loss = 0.473 Acc = 1.00\n",
      "Epoch 126 / 300: Loss = 0.466 Acc = 1.00\n",
      "Epoch 127 / 300: Loss = 0.459 Acc = 1.00\n",
      "Epoch 128 / 300: Loss = 0.452 Acc = 1.00\n",
      "Epoch 129 / 300: Loss = 0.445 Acc = 1.00\n",
      "Epoch 130 / 300: Loss = 0.438 Acc = 1.00\n",
      "Epoch 131 / 300: Loss = 0.432 Acc = 1.00\n",
      "Epoch 132 / 300: Loss = 0.425 Acc = 1.00\n",
      "Epoch 133 / 300: Loss = 0.418 Acc = 1.00\n",
      "Epoch 134 / 300: Loss = 0.412 Acc = 1.00\n",
      "Epoch 135 / 300: Loss = 0.406 Acc = 1.00\n",
      "Epoch 136 / 300: Loss = 0.399 Acc = 1.00\n",
      "Epoch 137 / 300: Loss = 0.393 Acc = 1.00\n",
      "Epoch 138 / 300: Loss = 0.387 Acc = 1.00\n",
      "Epoch 139 / 300: Loss = 0.381 Acc = 1.00\n",
      "Epoch 140 / 300: Loss = 0.375 Acc = 1.00\n",
      "Epoch 141 / 300: Loss = 0.369 Acc = 1.00\n",
      "Epoch 142 / 300: Loss = 0.363 Acc = 1.00\n",
      "Epoch 143 / 300: Loss = 0.358 Acc = 1.00\n",
      "Epoch 144 / 300: Loss = 0.352 Acc = 1.00\n",
      "Epoch 145 / 300: Loss = 0.347 Acc = 1.00\n",
      "Epoch 146 / 300: Loss = 0.341 Acc = 1.00\n",
      "Epoch 147 / 300: Loss = 0.336 Acc = 1.00\n",
      "Epoch 148 / 300: Loss = 0.330 Acc = 1.00\n",
      "Epoch 149 / 300: Loss = 0.325 Acc = 1.00\n",
      "Epoch 150 / 300: Loss = 0.320 Acc = 1.00\n",
      "Epoch 151 / 300: Loss = 0.315 Acc = 1.00\n",
      "Epoch 152 / 300: Loss = 0.310 Acc = 1.00\n",
      "Epoch 153 / 300: Loss = 0.305 Acc = 1.00\n",
      "Epoch 154 / 300: Loss = 0.300 Acc = 1.00\n",
      "Epoch 155 / 300: Loss = 0.296 Acc = 1.00\n",
      "Epoch 156 / 300: Loss = 0.291 Acc = 1.00\n",
      "Epoch 157 / 300: Loss = 0.286 Acc = 1.00\n",
      "Epoch 158 / 300: Loss = 0.282 Acc = 1.00\n",
      "Epoch 159 / 300: Loss = 0.277 Acc = 1.00\n",
      "Epoch 160 / 300: Loss = 0.273 Acc = 1.00\n",
      "Epoch 161 / 300: Loss = 0.269 Acc = 1.00\n",
      "Epoch 162 / 300: Loss = 0.264 Acc = 1.00\n",
      "Epoch 163 / 300: Loss = 0.260 Acc = 1.00\n",
      "Epoch 164 / 300: Loss = 0.256 Acc = 1.00\n",
      "Epoch 165 / 300: Loss = 0.252 Acc = 1.00\n",
      "Epoch 166 / 300: Loss = 0.248 Acc = 1.00\n",
      "Epoch 167 / 300: Loss = 0.244 Acc = 1.00\n",
      "Epoch 168 / 300: Loss = 0.240 Acc = 1.00\n",
      "Epoch 169 / 300: Loss = 0.237 Acc = 1.00\n",
      "Epoch 170 / 300: Loss = 0.233 Acc = 1.00\n",
      "Epoch 171 / 300: Loss = 0.229 Acc = 1.00\n",
      "Epoch 172 / 300: Loss = 0.226 Acc = 1.00\n",
      "Epoch 173 / 300: Loss = 0.222 Acc = 1.00\n",
      "Epoch 174 / 300: Loss = 0.219 Acc = 1.00\n",
      "Epoch 175 / 300: Loss = 0.216 Acc = 1.00\n",
      "Epoch 176 / 300: Loss = 0.212 Acc = 1.00\n",
      "Epoch 177 / 300: Loss = 0.209 Acc = 1.00\n",
      "Epoch 178 / 300: Loss = 0.206 Acc = 1.00\n",
      "Epoch 179 / 300: Loss = 0.203 Acc = 1.00\n",
      "Epoch 180 / 300: Loss = 0.200 Acc = 1.00\n",
      "Epoch 181 / 300: Loss = 0.197 Acc = 1.00\n",
      "Epoch 182 / 300: Loss = 0.194 Acc = 1.00\n",
      "Epoch 183 / 300: Loss = 0.191 Acc = 1.00\n",
      "Epoch 184 / 300: Loss = 0.188 Acc = 1.00\n",
      "Epoch 185 / 300: Loss = 0.185 Acc = 1.00\n",
      "Epoch 186 / 300: Loss = 0.183 Acc = 1.00\n",
      "Epoch 187 / 300: Loss = 0.180 Acc = 1.00\n",
      "Epoch 188 / 300: Loss = 0.177 Acc = 1.00\n",
      "Epoch 189 / 300: Loss = 0.175 Acc = 1.00\n",
      "Epoch 190 / 300: Loss = 0.172 Acc = 1.00\n",
      "Epoch 191 / 300: Loss = 0.170 Acc = 1.00\n",
      "Epoch 192 / 300: Loss = 0.167 Acc = 1.00\n",
      "Epoch 193 / 300: Loss = 0.165 Acc = 1.00\n",
      "Epoch 194 / 300: Loss = 0.162 Acc = 1.00\n",
      "Epoch 195 / 300: Loss = 0.160 Acc = 1.00\n",
      "Epoch 196 / 300: Loss = 0.158 Acc = 1.00\n",
      "Epoch 197 / 300: Loss = 0.156 Acc = 1.00\n",
      "Epoch 198 / 300: Loss = 0.154 Acc = 1.00\n",
      "Epoch 199 / 300: Loss = 0.151 Acc = 1.00\n",
      "Epoch 200 / 300: Loss = 0.149 Acc = 1.00\n",
      "Epoch 201 / 300: Loss = 0.147 Acc = 1.00\n",
      "Epoch 202 / 300: Loss = 0.145 Acc = 1.00\n",
      "Epoch 203 / 300: Loss = 0.143 Acc = 1.00\n",
      "Epoch 204 / 300: Loss = 0.141 Acc = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205 / 300: Loss = 0.139 Acc = 1.00\n",
      "Epoch 206 / 300: Loss = 0.138 Acc = 1.00\n",
      "Epoch 207 / 300: Loss = 0.136 Acc = 1.00\n",
      "Epoch 208 / 300: Loss = 0.134 Acc = 1.00\n",
      "Epoch 209 / 300: Loss = 0.132 Acc = 1.00\n",
      "Epoch 210 / 300: Loss = 0.130 Acc = 1.00\n",
      "Epoch 211 / 300: Loss = 0.129 Acc = 1.00\n",
      "Epoch 212 / 300: Loss = 0.127 Acc = 1.00\n",
      "Epoch 213 / 300: Loss = 0.125 Acc = 1.00\n",
      "Epoch 214 / 300: Loss = 0.124 Acc = 1.00\n",
      "Epoch 215 / 300: Loss = 0.122 Acc = 1.00\n",
      "Epoch 216 / 300: Loss = 0.121 Acc = 1.00\n",
      "Epoch 217 / 300: Loss = 0.119 Acc = 1.00\n",
      "Epoch 218 / 300: Loss = 0.118 Acc = 1.00\n",
      "Epoch 219 / 300: Loss = 0.116 Acc = 1.00\n",
      "Epoch 220 / 300: Loss = 0.115 Acc = 1.00\n",
      "Epoch 221 / 300: Loss = 0.113 Acc = 1.00\n",
      "Epoch 222 / 300: Loss = 0.112 Acc = 1.00\n",
      "Epoch 223 / 300: Loss = 0.111 Acc = 1.00\n",
      "Epoch 224 / 300: Loss = 0.109 Acc = 1.00\n",
      "Epoch 225 / 300: Loss = 0.108 Acc = 1.00\n",
      "Epoch 226 / 300: Loss = 0.107 Acc = 1.00\n",
      "Epoch 227 / 300: Loss = 0.106 Acc = 1.00\n",
      "Epoch 228 / 300: Loss = 0.104 Acc = 1.00\n",
      "Epoch 229 / 300: Loss = 0.103 Acc = 1.00\n",
      "Epoch 230 / 300: Loss = 0.102 Acc = 1.00\n",
      "Epoch 231 / 300: Loss = 0.101 Acc = 1.00\n",
      "Epoch 232 / 300: Loss = 0.100 Acc = 1.00\n",
      "Epoch 233 / 300: Loss = 0.099 Acc = 1.00\n",
      "Epoch 234 / 300: Loss = 0.097 Acc = 1.00\n",
      "Epoch 235 / 300: Loss = 0.096 Acc = 1.00\n",
      "Epoch 236 / 300: Loss = 0.095 Acc = 1.00\n",
      "Epoch 237 / 300: Loss = 0.094 Acc = 1.00\n",
      "Epoch 238 / 300: Loss = 0.093 Acc = 1.00\n",
      "Epoch 239 / 300: Loss = 0.092 Acc = 1.00\n",
      "Epoch 240 / 300: Loss = 0.091 Acc = 1.00\n",
      "Epoch 241 / 300: Loss = 0.090 Acc = 1.00\n",
      "Epoch 242 / 300: Loss = 0.089 Acc = 1.00\n",
      "Epoch 243 / 300: Loss = 0.088 Acc = 1.00\n",
      "Epoch 244 / 300: Loss = 0.087 Acc = 1.00\n",
      "Epoch 245 / 300: Loss = 0.086 Acc = 1.00\n",
      "Epoch 246 / 300: Loss = 0.086 Acc = 1.00\n",
      "Epoch 247 / 300: Loss = 0.085 Acc = 1.00\n",
      "Epoch 248 / 300: Loss = 0.084 Acc = 1.00\n",
      "Epoch 249 / 300: Loss = 0.083 Acc = 1.00\n",
      "Epoch 250 / 300: Loss = 0.082 Acc = 1.00\n",
      "Epoch 251 / 300: Loss = 0.081 Acc = 1.00\n",
      "Epoch 252 / 300: Loss = 0.081 Acc = 1.00\n",
      "Epoch 253 / 300: Loss = 0.080 Acc = 1.00\n",
      "Epoch 254 / 300: Loss = 0.079 Acc = 1.00\n",
      "Epoch 255 / 300: Loss = 0.078 Acc = 1.00\n",
      "Epoch 256 / 300: Loss = 0.077 Acc = 1.00\n",
      "Epoch 257 / 300: Loss = 0.077 Acc = 1.00\n",
      "Epoch 258 / 300: Loss = 0.076 Acc = 1.00\n",
      "Epoch 259 / 300: Loss = 0.075 Acc = 1.00\n",
      "Epoch 260 / 300: Loss = 0.074 Acc = 1.00\n",
      "Epoch 261 / 300: Loss = 0.074 Acc = 1.00\n",
      "Epoch 262 / 300: Loss = 0.073 Acc = 1.00\n",
      "Epoch 263 / 300: Loss = 0.072 Acc = 1.00\n",
      "Epoch 264 / 300: Loss = 0.072 Acc = 1.00\n",
      "Epoch 265 / 300: Loss = 0.071 Acc = 1.00\n",
      "Epoch 266 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 267 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 268 / 300: Loss = 0.069 Acc = 1.00\n",
      "Epoch 269 / 300: Loss = 0.069 Acc = 1.00\n",
      "Epoch 270 / 300: Loss = 0.068 Acc = 1.00\n",
      "Epoch 271 / 300: Loss = 0.067 Acc = 1.00\n",
      "Epoch 272 / 300: Loss = 0.067 Acc = 1.00\n",
      "Epoch 273 / 300: Loss = 0.066 Acc = 1.00\n",
      "Epoch 274 / 300: Loss = 0.066 Acc = 1.00\n",
      "Epoch 275 / 300: Loss = 0.065 Acc = 1.00\n",
      "Epoch 276 / 300: Loss = 0.064 Acc = 1.00\n",
      "Epoch 277 / 300: Loss = 0.064 Acc = 1.00\n",
      "Epoch 278 / 300: Loss = 0.063 Acc = 1.00\n",
      "Epoch 279 / 300: Loss = 0.063 Acc = 1.00\n",
      "Epoch 280 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 281 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 282 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 283 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 284 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 285 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 286 / 300: Loss = 0.059 Acc = 1.00\n",
      "Epoch 287 / 300: Loss = 0.059 Acc = 1.00\n",
      "Epoch 288 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 289 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 290 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 291 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 292 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 293 / 300: Loss = 0.056 Acc = 1.00\n",
      "Epoch 294 / 300: Loss = 0.056 Acc = 1.00\n",
      "Epoch 295 / 300: Loss = 0.055 Acc = 1.00\n",
      "Epoch 296 / 300: Loss = 0.055 Acc = 1.00\n",
      "Epoch 297 / 300: Loss = 0.054 Acc = 1.00\n",
      "Epoch 298 / 300: Loss = 0.054 Acc = 1.00\n",
      "Epoch 299 / 300: Loss = 0.054 Acc = 1.00\n",
      "Epoch 300 / 300: Loss = 0.053 Acc = 1.00\n"
     ]
    }
   ],
   "source": [
    "history_classical = train(model_classical, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(model):\n",
    "    with torch.no_grad():\n",
    "        input_sentence = training_data[0][0]\n",
    "        labels = training_data[0][1]\n",
    "        inputs = prepare_sequence(input_sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        tag_ids = torch.argmax(tag_scores, dim=1).numpy()\n",
    "        tag_labels = [ix_to_tag[k] for k in tag_ids]\n",
    "        print(f\"Sentence:  {input_sentence}\")\n",
    "        print(f\"Labels:    {labels}\")\n",
    "        print(f\"Predicted: {tag_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Labels:    ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "Predicted: ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "print_result(model_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger will use Quantum LSTM\n",
      "weight_shapes = (n_qlayers, n_qubits) = (1, 4)\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "\n",
    "model_quantum = LSTMTagger(embedding_dim, \n",
    "                        hidden_dim, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(tag_to_ix), \n",
    "                        n_qubits=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300: Loss = 1.086 Acc = 0.33\n",
      "Epoch 2 / 300: Loss = 1.081 Acc = 0.33\n",
      "Epoch 3 / 300: Loss = 1.077 Acc = 0.33\n",
      "Epoch 4 / 300: Loss = 1.073 Acc = 0.33\n",
      "Epoch 5 / 300: Loss = 1.070 Acc = 0.33\n",
      "Epoch 6 / 300: Loss = 1.067 Acc = 0.33\n",
      "Epoch 7 / 300: Loss = 1.065 Acc = 0.44\n",
      "Epoch 8 / 300: Loss = 1.062 Acc = 0.44\n",
      "Epoch 9 / 300: Loss = 1.060 Acc = 0.44\n",
      "Epoch 10 / 300: Loss = 1.058 Acc = 0.44\n",
      "Epoch 11 / 300: Loss = 1.056 Acc = 0.44\n",
      "Epoch 12 / 300: Loss = 1.055 Acc = 0.44\n",
      "Epoch 13 / 300: Loss = 1.053 Acc = 0.44\n",
      "Epoch 14 / 300: Loss = 1.051 Acc = 0.44\n",
      "Epoch 15 / 300: Loss = 1.049 Acc = 0.44\n",
      "Epoch 16 / 300: Loss = 1.048 Acc = 0.44\n",
      "Epoch 17 / 300: Loss = 1.046 Acc = 0.44\n",
      "Epoch 18 / 300: Loss = 1.044 Acc = 0.44\n",
      "Epoch 19 / 300: Loss = 1.042 Acc = 0.44\n",
      "Epoch 20 / 300: Loss = 1.040 Acc = 0.44\n",
      "Epoch 21 / 300: Loss = 1.037 Acc = 0.44\n",
      "Epoch 22 / 300: Loss = 1.035 Acc = 0.44\n",
      "Epoch 23 / 300: Loss = 1.032 Acc = 0.44\n",
      "Epoch 24 / 300: Loss = 1.030 Acc = 0.44\n",
      "Epoch 25 / 300: Loss = 1.027 Acc = 0.44\n",
      "Epoch 26 / 300: Loss = 1.024 Acc = 0.44\n",
      "Epoch 27 / 300: Loss = 1.020 Acc = 0.44\n",
      "Epoch 28 / 300: Loss = 1.017 Acc = 0.44\n",
      "Epoch 29 / 300: Loss = 1.013 Acc = 0.44\n",
      "Epoch 30 / 300: Loss = 1.009 Acc = 0.44\n",
      "Epoch 31 / 300: Loss = 1.005 Acc = 0.44\n",
      "Epoch 32 / 300: Loss = 1.000 Acc = 0.44\n",
      "Epoch 33 / 300: Loss = 0.996 Acc = 0.44\n",
      "Epoch 34 / 300: Loss = 0.991 Acc = 0.44\n",
      "Epoch 35 / 300: Loss = 0.986 Acc = 0.44\n",
      "Epoch 36 / 300: Loss = 0.981 Acc = 0.44\n",
      "Epoch 37 / 300: Loss = 0.975 Acc = 0.44\n",
      "Epoch 38 / 300: Loss = 0.970 Acc = 0.44\n",
      "Epoch 39 / 300: Loss = 0.964 Acc = 0.44\n",
      "Epoch 40 / 300: Loss = 0.959 Acc = 0.44\n",
      "Epoch 41 / 300: Loss = 0.953 Acc = 0.44\n",
      "Epoch 42 / 300: Loss = 0.947 Acc = 0.44\n",
      "Epoch 43 / 300: Loss = 0.941 Acc = 0.44\n",
      "Epoch 44 / 300: Loss = 0.935 Acc = 0.44\n",
      "Epoch 45 / 300: Loss = 0.929 Acc = 0.44\n",
      "Epoch 46 / 300: Loss = 0.923 Acc = 0.44\n",
      "Epoch 47 / 300: Loss = 0.916 Acc = 0.56\n",
      "Epoch 48 / 300: Loss = 0.910 Acc = 0.56\n",
      "Epoch 49 / 300: Loss = 0.904 Acc = 0.56\n",
      "Epoch 50 / 300: Loss = 0.897 Acc = 0.67\n",
      "Epoch 51 / 300: Loss = 0.891 Acc = 0.67\n",
      "Epoch 52 / 300: Loss = 0.885 Acc = 0.67\n",
      "Epoch 53 / 300: Loss = 0.878 Acc = 0.67\n",
      "Epoch 54 / 300: Loss = 0.872 Acc = 0.67\n",
      "Epoch 55 / 300: Loss = 0.866 Acc = 0.67\n",
      "Epoch 56 / 300: Loss = 0.859 Acc = 0.67\n",
      "Epoch 57 / 300: Loss = 0.853 Acc = 0.67\n",
      "Epoch 58 / 300: Loss = 0.847 Acc = 0.67\n",
      "Epoch 59 / 300: Loss = 0.840 Acc = 0.67\n",
      "Epoch 60 / 300: Loss = 0.834 Acc = 0.67\n",
      "Epoch 61 / 300: Loss = 0.828 Acc = 0.67\n",
      "Epoch 62 / 300: Loss = 0.822 Acc = 0.67\n",
      "Epoch 63 / 300: Loss = 0.816 Acc = 0.67\n",
      "Epoch 64 / 300: Loss = 0.809 Acc = 0.67\n",
      "Epoch 65 / 300: Loss = 0.803 Acc = 0.67\n",
      "Epoch 66 / 300: Loss = 0.797 Acc = 0.67\n",
      "Epoch 67 / 300: Loss = 0.791 Acc = 0.67\n",
      "Epoch 68 / 300: Loss = 0.785 Acc = 0.67\n",
      "Epoch 69 / 300: Loss = 0.779 Acc = 0.67\n",
      "Epoch 70 / 300: Loss = 0.774 Acc = 0.67\n",
      "Epoch 71 / 300: Loss = 0.768 Acc = 0.67\n",
      "Epoch 72 / 300: Loss = 0.762 Acc = 0.67\n",
      "Epoch 73 / 300: Loss = 0.757 Acc = 0.67\n",
      "Epoch 74 / 300: Loss = 0.751 Acc = 0.67\n",
      "Epoch 75 / 300: Loss = 0.746 Acc = 0.67\n",
      "Epoch 76 / 300: Loss = 0.740 Acc = 0.67\n",
      "Epoch 77 / 300: Loss = 0.735 Acc = 0.67\n",
      "Epoch 78 / 300: Loss = 0.730 Acc = 0.67\n",
      "Epoch 79 / 300: Loss = 0.725 Acc = 0.67\n",
      "Epoch 80 / 300: Loss = 0.720 Acc = 0.67\n",
      "Epoch 81 / 300: Loss = 0.715 Acc = 0.67\n",
      "Epoch 82 / 300: Loss = 0.710 Acc = 0.67\n",
      "Epoch 83 / 300: Loss = 0.706 Acc = 0.67\n",
      "Epoch 84 / 300: Loss = 0.701 Acc = 0.67\n",
      "Epoch 85 / 300: Loss = 0.696 Acc = 0.67\n",
      "Epoch 86 / 300: Loss = 0.692 Acc = 0.67\n",
      "Epoch 87 / 300: Loss = 0.688 Acc = 0.67\n",
      "Epoch 88 / 300: Loss = 0.683 Acc = 0.67\n",
      "Epoch 89 / 300: Loss = 0.679 Acc = 0.67\n",
      "Epoch 90 / 300: Loss = 0.675 Acc = 0.67\n",
      "Epoch 91 / 300: Loss = 0.671 Acc = 0.67\n",
      "Epoch 92 / 300: Loss = 0.667 Acc = 0.67\n",
      "Epoch 93 / 300: Loss = 0.663 Acc = 0.67\n",
      "Epoch 94 / 300: Loss = 0.659 Acc = 0.67\n",
      "Epoch 95 / 300: Loss = 0.655 Acc = 0.67\n",
      "Epoch 96 / 300: Loss = 0.651 Acc = 0.67\n",
      "Epoch 97 / 300: Loss = 0.648 Acc = 0.67\n",
      "Epoch 98 / 300: Loss = 0.644 Acc = 0.67\n",
      "Epoch 99 / 300: Loss = 0.640 Acc = 0.67\n",
      "Epoch 100 / 300: Loss = 0.636 Acc = 0.67\n",
      "Epoch 101 / 300: Loss = 0.632 Acc = 0.67\n",
      "Epoch 102 / 300: Loss = 0.628 Acc = 0.67\n",
      "Epoch 103 / 300: Loss = 0.623 Acc = 0.67\n",
      "Epoch 104 / 300: Loss = 0.619 Acc = 0.67\n",
      "Epoch 105 / 300: Loss = 0.615 Acc = 0.67\n",
      "Epoch 106 / 300: Loss = 0.610 Acc = 0.67\n",
      "Epoch 107 / 300: Loss = 0.606 Acc = 0.67\n",
      "Epoch 108 / 300: Loss = 0.601 Acc = 0.67\n",
      "Epoch 109 / 300: Loss = 0.597 Acc = 0.67\n",
      "Epoch 110 / 300: Loss = 0.593 Acc = 0.67\n",
      "Epoch 111 / 300: Loss = 0.589 Acc = 0.67\n",
      "Epoch 112 / 300: Loss = 0.585 Acc = 0.67\n",
      "Epoch 113 / 300: Loss = 0.581 Acc = 0.67\n",
      "Epoch 114 / 300: Loss = 0.577 Acc = 0.67\n",
      "Epoch 115 / 300: Loss = 0.573 Acc = 0.67\n",
      "Epoch 116 / 300: Loss = 0.569 Acc = 0.67\n",
      "Epoch 117 / 300: Loss = 0.565 Acc = 0.78\n",
      "Epoch 118 / 300: Loss = 0.561 Acc = 0.78\n",
      "Epoch 119 / 300: Loss = 0.557 Acc = 0.78\n",
      "Epoch 120 / 300: Loss = 0.553 Acc = 0.78\n",
      "Epoch 121 / 300: Loss = 0.549 Acc = 0.78\n",
      "Epoch 122 / 300: Loss = 0.545 Acc = 0.78\n",
      "Epoch 123 / 300: Loss = 0.541 Acc = 0.78\n",
      "Epoch 124 / 300: Loss = 0.537 Acc = 0.78\n",
      "Epoch 125 / 300: Loss = 0.533 Acc = 0.78\n",
      "Epoch 126 / 300: Loss = 0.528 Acc = 0.78\n",
      "Epoch 127 / 300: Loss = 0.524 Acc = 0.78\n",
      "Epoch 128 / 300: Loss = 0.520 Acc = 0.78\n",
      "Epoch 129 / 300: Loss = 0.516 Acc = 0.78\n",
      "Epoch 130 / 300: Loss = 0.511 Acc = 0.78\n",
      "Epoch 131 / 300: Loss = 0.507 Acc = 0.78\n",
      "Epoch 132 / 300: Loss = 0.502 Acc = 0.78\n",
      "Epoch 133 / 300: Loss = 0.498 Acc = 0.78\n",
      "Epoch 134 / 300: Loss = 0.493 Acc = 0.78\n",
      "Epoch 135 / 300: Loss = 0.488 Acc = 0.78\n",
      "Epoch 136 / 300: Loss = 0.483 Acc = 0.78\n",
      "Epoch 137 / 300: Loss = 0.478 Acc = 0.78\n",
      "Epoch 138 / 300: Loss = 0.473 Acc = 0.78\n",
      "Epoch 139 / 300: Loss = 0.468 Acc = 0.78\n",
      "Epoch 140 / 300: Loss = 0.463 Acc = 0.78\n",
      "Epoch 141 / 300: Loss = 0.458 Acc = 0.78\n",
      "Epoch 142 / 300: Loss = 0.453 Acc = 0.78\n",
      "Epoch 143 / 300: Loss = 0.447 Acc = 0.89\n",
      "Epoch 144 / 300: Loss = 0.442 Acc = 0.89\n",
      "Epoch 145 / 300: Loss = 0.437 Acc = 0.89\n",
      "Epoch 146 / 300: Loss = 0.431 Acc = 0.89\n",
      "Epoch 147 / 300: Loss = 0.425 Acc = 0.89\n",
      "Epoch 148 / 300: Loss = 0.419 Acc = 0.89\n",
      "Epoch 149 / 300: Loss = 0.413 Acc = 0.89\n",
      "Epoch 150 / 300: Loss = 0.407 Acc = 0.89\n",
      "Epoch 151 / 300: Loss = 0.400 Acc = 0.89\n",
      "Epoch 152 / 300: Loss = 0.393 Acc = 0.89\n",
      "Epoch 153 / 300: Loss = 0.385 Acc = 1.00\n",
      "Epoch 154 / 300: Loss = 0.378 Acc = 1.00\n",
      "Epoch 155 / 300: Loss = 0.370 Acc = 1.00\n",
      "Epoch 156 / 300: Loss = 0.362 Acc = 1.00\n",
      "Epoch 157 / 300: Loss = 0.354 Acc = 1.00\n",
      "Epoch 158 / 300: Loss = 0.346 Acc = 1.00\n",
      "Epoch 159 / 300: Loss = 0.339 Acc = 1.00\n",
      "Epoch 160 / 300: Loss = 0.332 Acc = 1.00\n",
      "Epoch 161 / 300: Loss = 0.325 Acc = 1.00\n",
      "Epoch 162 / 300: Loss = 0.318 Acc = 1.00\n",
      "Epoch 163 / 300: Loss = 0.312 Acc = 1.00\n",
      "Epoch 164 / 300: Loss = 0.306 Acc = 1.00\n",
      "Epoch 165 / 300: Loss = 0.300 Acc = 1.00\n",
      "Epoch 166 / 300: Loss = 0.294 Acc = 1.00\n",
      "Epoch 167 / 300: Loss = 0.289 Acc = 1.00\n",
      "Epoch 168 / 300: Loss = 0.283 Acc = 1.00\n",
      "Epoch 169 / 300: Loss = 0.278 Acc = 1.00\n",
      "Epoch 170 / 300: Loss = 0.273 Acc = 1.00\n",
      "Epoch 171 / 300: Loss = 0.268 Acc = 1.00\n",
      "Epoch 172 / 300: Loss = 0.263 Acc = 1.00\n",
      "Epoch 173 / 300: Loss = 0.258 Acc = 1.00\n",
      "Epoch 174 / 300: Loss = 0.254 Acc = 1.00\n",
      "Epoch 175 / 300: Loss = 0.249 Acc = 1.00\n",
      "Epoch 176 / 300: Loss = 0.244 Acc = 1.00\n",
      "Epoch 177 / 300: Loss = 0.240 Acc = 1.00\n",
      "Epoch 178 / 300: Loss = 0.236 Acc = 1.00\n",
      "Epoch 179 / 300: Loss = 0.231 Acc = 1.00\n",
      "Epoch 180 / 300: Loss = 0.227 Acc = 1.00\n",
      "Epoch 181 / 300: Loss = 0.223 Acc = 1.00\n",
      "Epoch 182 / 300: Loss = 0.219 Acc = 1.00\n",
      "Epoch 183 / 300: Loss = 0.214 Acc = 1.00\n",
      "Epoch 184 / 300: Loss = 0.210 Acc = 1.00\n",
      "Epoch 185 / 300: Loss = 0.206 Acc = 1.00\n",
      "Epoch 186 / 300: Loss = 0.202 Acc = 1.00\n",
      "Epoch 187 / 300: Loss = 0.198 Acc = 1.00\n",
      "Epoch 188 / 300: Loss = 0.194 Acc = 1.00\n",
      "Epoch 189 / 300: Loss = 0.190 Acc = 1.00\n",
      "Epoch 190 / 300: Loss = 0.186 Acc = 1.00\n",
      "Epoch 191 / 300: Loss = 0.182 Acc = 1.00\n",
      "Epoch 192 / 300: Loss = 0.178 Acc = 1.00\n",
      "Epoch 193 / 300: Loss = 0.175 Acc = 1.00\n",
      "Epoch 194 / 300: Loss = 0.171 Acc = 1.00\n",
      "Epoch 195 / 300: Loss = 0.168 Acc = 1.00\n",
      "Epoch 196 / 300: Loss = 0.165 Acc = 1.00\n",
      "Epoch 197 / 300: Loss = 0.162 Acc = 1.00\n",
      "Epoch 198 / 300: Loss = 0.159 Acc = 1.00\n",
      "Epoch 199 / 300: Loss = 0.157 Acc = 1.00\n",
      "Epoch 200 / 300: Loss = 0.154 Acc = 1.00\n",
      "Epoch 201 / 300: Loss = 0.152 Acc = 1.00\n",
      "Epoch 202 / 300: Loss = 0.149 Acc = 1.00\n",
      "Epoch 203 / 300: Loss = 0.147 Acc = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 300: Loss = 0.145 Acc = 1.00\n",
      "Epoch 205 / 300: Loss = 0.143 Acc = 1.00\n",
      "Epoch 206 / 300: Loss = 0.141 Acc = 1.00\n",
      "Epoch 207 / 300: Loss = 0.139 Acc = 1.00\n",
      "Epoch 208 / 300: Loss = 0.137 Acc = 1.00\n",
      "Epoch 209 / 300: Loss = 0.135 Acc = 1.00\n",
      "Epoch 210 / 300: Loss = 0.133 Acc = 1.00\n",
      "Epoch 211 / 300: Loss = 0.131 Acc = 1.00\n",
      "Epoch 212 / 300: Loss = 0.129 Acc = 1.00\n",
      "Epoch 213 / 300: Loss = 0.128 Acc = 1.00\n",
      "Epoch 214 / 300: Loss = 0.126 Acc = 1.00\n",
      "Epoch 215 / 300: Loss = 0.124 Acc = 1.00\n",
      "Epoch 216 / 300: Loss = 0.123 Acc = 1.00\n",
      "Epoch 217 / 300: Loss = 0.121 Acc = 1.00\n",
      "Epoch 218 / 300: Loss = 0.120 Acc = 1.00\n",
      "Epoch 219 / 300: Loss = 0.118 Acc = 1.00\n",
      "Epoch 220 / 300: Loss = 0.117 Acc = 1.00\n",
      "Epoch 221 / 300: Loss = 0.115 Acc = 1.00\n",
      "Epoch 222 / 300: Loss = 0.114 Acc = 1.00\n",
      "Epoch 223 / 300: Loss = 0.113 Acc = 1.00\n",
      "Epoch 224 / 300: Loss = 0.111 Acc = 1.00\n",
      "Epoch 225 / 300: Loss = 0.110 Acc = 1.00\n",
      "Epoch 226 / 300: Loss = 0.109 Acc = 1.00\n",
      "Epoch 227 / 300: Loss = 0.107 Acc = 1.00\n",
      "Epoch 228 / 300: Loss = 0.106 Acc = 1.00\n",
      "Epoch 229 / 300: Loss = 0.105 Acc = 1.00\n",
      "Epoch 230 / 300: Loss = 0.104 Acc = 1.00\n",
      "Epoch 231 / 300: Loss = 0.103 Acc = 1.00\n",
      "Epoch 232 / 300: Loss = 0.101 Acc = 1.00\n",
      "Epoch 233 / 300: Loss = 0.100 Acc = 1.00\n",
      "Epoch 234 / 300: Loss = 0.099 Acc = 1.00\n",
      "Epoch 235 / 300: Loss = 0.098 Acc = 1.00\n",
      "Epoch 236 / 300: Loss = 0.097 Acc = 1.00\n",
      "Epoch 237 / 300: Loss = 0.096 Acc = 1.00\n",
      "Epoch 238 / 300: Loss = 0.095 Acc = 1.00\n",
      "Epoch 239 / 300: Loss = 0.094 Acc = 1.00\n",
      "Epoch 240 / 300: Loss = 0.093 Acc = 1.00\n",
      "Epoch 241 / 300: Loss = 0.092 Acc = 1.00\n",
      "Epoch 242 / 300: Loss = 0.091 Acc = 1.00\n",
      "Epoch 243 / 300: Loss = 0.090 Acc = 1.00\n",
      "Epoch 244 / 300: Loss = 0.089 Acc = 1.00\n",
      "Epoch 245 / 300: Loss = 0.089 Acc = 1.00\n",
      "Epoch 246 / 300: Loss = 0.088 Acc = 1.00\n",
      "Epoch 247 / 300: Loss = 0.087 Acc = 1.00\n",
      "Epoch 248 / 300: Loss = 0.086 Acc = 1.00\n",
      "Epoch 249 / 300: Loss = 0.085 Acc = 1.00\n",
      "Epoch 250 / 300: Loss = 0.084 Acc = 1.00\n",
      "Epoch 251 / 300: Loss = 0.084 Acc = 1.00\n",
      "Epoch 252 / 300: Loss = 0.083 Acc = 1.00\n",
      "Epoch 253 / 300: Loss = 0.082 Acc = 1.00\n",
      "Epoch 254 / 300: Loss = 0.081 Acc = 1.00\n",
      "Epoch 255 / 300: Loss = 0.081 Acc = 1.00\n",
      "Epoch 256 / 300: Loss = 0.080 Acc = 1.00\n",
      "Epoch 257 / 300: Loss = 0.079 Acc = 1.00\n",
      "Epoch 258 / 300: Loss = 0.078 Acc = 1.00\n",
      "Epoch 259 / 300: Loss = 0.078 Acc = 1.00\n",
      "Epoch 260 / 300: Loss = 0.077 Acc = 1.00\n",
      "Epoch 261 / 300: Loss = 0.076 Acc = 1.00\n",
      "Epoch 262 / 300: Loss = 0.076 Acc = 1.00\n",
      "Epoch 263 / 300: Loss = 0.075 Acc = 1.00\n",
      "Epoch 264 / 300: Loss = 0.074 Acc = 1.00\n",
      "Epoch 265 / 300: Loss = 0.074 Acc = 1.00\n",
      "Epoch 266 / 300: Loss = 0.073 Acc = 1.00\n",
      "Epoch 267 / 300: Loss = 0.072 Acc = 1.00\n",
      "Epoch 268 / 300: Loss = 0.072 Acc = 1.00\n",
      "Epoch 269 / 300: Loss = 0.071 Acc = 1.00\n",
      "Epoch 270 / 300: Loss = 0.071 Acc = 1.00\n",
      "Epoch 271 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 272 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 273 / 300: Loss = 0.069 Acc = 1.00\n",
      "Epoch 274 / 300: Loss = 0.068 Acc = 1.00\n",
      "Epoch 275 / 300: Loss = 0.068 Acc = 1.00\n",
      "Epoch 276 / 300: Loss = 0.067 Acc = 1.00\n",
      "Epoch 277 / 300: Loss = 0.067 Acc = 1.00\n",
      "Epoch 278 / 300: Loss = 0.066 Acc = 1.00\n",
      "Epoch 279 / 300: Loss = 0.066 Acc = 1.00\n",
      "Epoch 280 / 300: Loss = 0.065 Acc = 1.00\n",
      "Epoch 281 / 300: Loss = 0.065 Acc = 1.00\n",
      "Epoch 282 / 300: Loss = 0.064 Acc = 1.00\n",
      "Epoch 283 / 300: Loss = 0.064 Acc = 1.00\n",
      "Epoch 284 / 300: Loss = 0.063 Acc = 1.00\n",
      "Epoch 285 / 300: Loss = 0.063 Acc = 1.00\n",
      "Epoch 286 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 287 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 288 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 289 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 290 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 291 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 292 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 293 / 300: Loss = 0.059 Acc = 1.00\n",
      "Epoch 294 / 300: Loss = 0.059 Acc = 1.00\n",
      "Epoch 295 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 296 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 297 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 298 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 299 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 300 / 300: Loss = 0.056 Acc = 1.00\n"
     ]
    }
   ],
   "source": [
    "history_quantum = train(model_quantum, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Labels:    ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "Predicted: ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "print_result(model_quantum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history_classical, history_quantum):\n",
    "    loss_c = history_classical['loss']\n",
    "    acc_c = history_classical['acc']\n",
    "    loss_q = history_quantum['loss']\n",
    "    acc_q = history_quantum['acc']\n",
    "    n_epochs = max([len(loss_c), len(loss_q)])\n",
    "    x_epochs = [i for i in range(n_epochs)]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.plot(loss_c, label=\"Classical LSTM loss\", color='orange', linestyle='dashed')\n",
    "    ax1.plot(loss_q, label=\"Quantum LSTM loss\", color='red', linestyle='solid')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.plot(acc_c, label=\"Classical LSTM accuracy\", color='steelblue', linestyle='dashed')\n",
    "    ax2.plot(acc_q, label=\"Quantum LSTM accuracy\", color='blue', linestyle='solid')\n",
    "\n",
    "    plt.title(\"Part-of-Speech Tagger Training\")\n",
    "    plt.ylim(0., 1.1)\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1,0.8), bbox_transform=ax1.transAxes)\n",
    "\n",
    "    plt.savefig(\"pos_training.pdf\")\n",
    "    plt.savefig(\"pos_training.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history_classical, history_quantum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function decreases as a function of the training epoch, and after 300 epochs both networks are able to tag correctly the first sentence. Due to the complexity of the simulation of the quantum circuit, it took approximatively 15 minutes to finish the training, to be compared to a mere 8 seconds for the classical case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
